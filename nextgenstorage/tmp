
package httpclient.spark;

import httpclient.base.BaseHttpClientHelper.METHOD;

import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.IOException;

import model.spark.PersistentContextOptions;
import model.spark.StartJobOptions;

import org.apache.http.HttpEntity;
import org.apache.http.entity.InputStreamEntity;
import org.apache.http.entity.StringEntity;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.base.Preconditions;

public class SparkAccess {

	private final static Logger LOGGER = LoggerFactory.getLogger(SparkAccess.class);
	
	private SparkHttpClientHelper sparkHttpClientHelper;
	
	public SparkAccess(SparkHttpAddress sparkHttpAddress) {
		this.sparkHttpClientHelper = new SparkHttpClientHelper(sparkHttpAddress);
	}
	
	public SparkAccess(String host, String port) {
		this.sparkHttpClientHelper = new SparkHttpClientHelper(new SparkHttpAddress(host, port));
	}
	
	public SparkAccess(String protocol, String host, String port) {
		this.sparkHttpClientHelper = new SparkHttpClientHelper(new SparkHttpAddress(protocol, host, port));
	}
	
	private final class LITERAL {
		private static final String JARS = "jars";
		private static final String JAR_SUFFIX = ".jar";
		private static final String CONTEXTS = "contexts";
		private static final String JOBS = "jobs";
		private static final String CONFIG = "config";
		private static final String DATA = "data";
		
		private final class ERROR {
			private static final String UPLOAD_ERROR01 = "Failed to upload a directory";
			private static final String UPLOAD_ERROR02 = "Failed to upload a not jar type file";
		}
	}
	
	/**
	 * [GET] Lists all the jars and the last upload Timestamp.
	 * @return
	 * @throws UnsupportedOperationException
	 * @throws IOException
	 */
	public String getJars() throws UnsupportedOperationException, IOException {
		LOGGER.info("Get jars.");
		return sparkHttpClientHelper
				.createSparkURL(LITERAL.JARS)
				.getHttpResultContent(METHOD.GET);
	}
	
	/**
	 * [POST] Uploads a new jar under <appName>
	 * @param localJarPath
	 * @param appName
	 * @return
	 * @throws UnsupportedOperationException
	 * @throws IOException
	 */
	public String uploadJar(String localJarPath, String appName) throws UnsupportedOperationException, IOException {
		LOGGER.info("Upload jars: Jar local path is {} with application name {}.", localJarPath, appName);
		return sparkHttpClientHelper
				.createSparkURL(LITERAL.JARS, appName)
				.getHttpResultContent(METHOD.POST, getHttpEntityWithJar(localJarPath));
	}
	
	/**
	 * [GET] Lists all current contexts.
	 * @return
	 * @throws UnsupportedOperationException
	 * @throws IOException
	 */
	public String getContexts() throws UnsupportedOperationException, IOException {
		LOGGER.info("Get context.");
		return sparkHttpClientHelper
				.createSparkURL(LITERAL.CONTEXTS)
				.getHttpResultContent(METHOD.GET);
	}
	
	/**
	 * [POST] Creates a new context
	 * TODO: Need to verify dependent-jar-uris
	 * @param contextName
	 * @param options
	 * @return
	 * @throws UnsupportedOperationException
	 * @throws IOException
	 */
	public String persistentContext(String contextName, PersistentContextOptions options) throws UnsupportedOperationException, IOException {
		LOGGER.info("Persistent context {}.", contextName);
		return sparkHttpClientHelper
				.createSparkURL(options, LITERAL.CONTEXTS, contextName)
				.getHttpResultContent(METHOD.POST);
	}
	
	/**
	 * [DELETE] Stops a context and all jobs running in it.
	 * @param contextName
	 * @return
	 * @throws UnsupportedOperationException
	 * @throws IOException
	 */
	public String stopContext(String contextName) throws UnsupportedOperationException, IOException {
		LOGGER.info("Stop context {}.", contextName);
		return sparkHttpClientHelper
				.createSparkURL(LITERAL.CONTEXTS, contextName)
				.getHttpResultContent(METHOD.DELETE);
	}
	
	/**
	 * [PUT] Kills all contexts and re-loads only the contexts from config
	 * @return
	 * @throws UnsupportedOperationException
	 * @throws IOException
	 */
	public String resetContexts() throws UnsupportedOperationException, IOException {
		LOGGER.info("Reset context.");
		return sparkHttpClientHelper
				.createSparkURL(LITERAL.CONTEXTS, "?reset=reboot")
				.getHttpResultContent(METHOD.PUT);
	}
	
	/**
	 * [GET] Lists all jobs
	 * @return
	 * @throws UnsupportedOperationException
	 * @throws IOException
	 */
	public String getJobs() throws UnsupportedOperationException, IOException {
		LOGGER.info("Get jobs.");
		return sparkHttpClientHelper
				.createSparkURL(LITERAL.JOBS)
				.getHttpResultContent(METHOD.GET);
	}
	
	/**
	 * [GET] Lists a job by job id
	 * @param jobId
	 * @return
	 * @throws UnsupportedOperationException
	 * @throws IOException
	 */
	public String getJob(String jobId) throws UnsupportedOperationException, IOException {
		LOGGER.info("Get job {}.", jobId);
		return sparkHttpClientHelper
				.createSparkURL(LITERAL.JOBS, jobId)
				.getHttpResultContent(METHOD.GET);
	}
	
	/**
	 * [POST] Starts a new job without data.
	 * @param startJobOptions
	 * @return
	 * @throws UnsupportedOperationException
	 * @throws IOException
	 */
	public String startJob(StartJobOptions startJobOptions) throws UnsupportedOperationException, IOException {
		LOGGER.info("Start job, appName is {} and classPath is {}", startJobOptions.getAppName(), startJobOptions.getClassPath());
		return startJob(startJobOptions, null);
	}
	
	/**
	 * [POST] Starts a new job with data()
	 * * For files that are larger than a few hundred MB, 
	 * * it is recommended to manually upload these files 
	 * * to the server or to directly add them to your HDFS.
	 * @param startJobOptions
	 * @param dataEntity
	 * @return
	 * @throws UnsupportedOperationException
	 * @throws IOException
	 */
	public String startJob(StartJobOptions startJobOptions, HttpEntity dataEntity) throws UnsupportedOperationException, IOException {
		LOGGER.info("Start job wit HttpEntity, appName is {} and classPath is {}", startJobOptions.getAppName(), startJobOptions.getClassPath());
		return sparkHttpClientHelper
				.createSparkURL(LITERAL.JOBS, startJobOptions.toString())
				.getHttpResultContent(METHOD.POST, dataEntity);
	}
	
	/**
	 * [GET] Gets the job configuration
	 * @param jobId
	 * @return
	 * @throws UnsupportedOperationException
	 * @throws IOException
	 */
	public String getJobConfig(String jobId) throws UnsupportedOperationException, IOException {
		LOGGER.info("Get job configuration.");
		return sparkHttpClientHelper
				.createSparkURL(LITERAL.JOBS, jobId, LITERAL.CONFIG)
				.getHttpResultContent(METHOD.GET);
	}
	
	/**
	 * [DELETE] Kills the specified job
	 * @param jobId
	 * @return
	 * @throws UnsupportedOperationException
	 * @throws IOException
	 */
	public String killJob(String jobId) throws UnsupportedOperationException, IOException {
		LOGGER.info("Kill job {}", jobId);
		return sparkHttpClientHelper
				.createSparkURL(LITERAL.JOBS, jobId)
				.getHttpResultContent(METHOD.DELETE);
	}
	
	/**
	 * [GET] Lists previously uploaded files that were not yet deleted
	 * @return
	 * @throws UnsupportedOperationException
	 * @throws IOException
	 */
	public String getData() throws UnsupportedOperationException, IOException {
		LOGGER.info("Get data.");
		return sparkHttpClientHelper
				.createSparkURL(LITERAL.DATA)
				.getHttpResultContent(METHOD.GET);
	}
	
	/**
	 * [POST] Uploads a new file, the full path of the file on the server is returned, 
	 * the prefix is the prefix of the actual filename used on the server (a TIMESTAMP is 
     * added to ensure uniqueness)         
	 * @param filePrefix
	 * @param localFilePath
	 * @return
	 * @throws UnsupportedOperationException
	 * @throws IOException
	 */
	public String uploadData(String filePrefix, String localFilePath) throws UnsupportedOperationException, IOException {
		LOGGER.info("Upload data from {} with prefix {}", localFilePath, filePrefix);
		File file = new File(localFilePath);
		Preconditions.checkArgument(file.exists(), "File does not exist");
		Preconditions.checkArgument(!file.isDirectory(), "Failed to upload a directory");
		InputStreamEntity entity = new InputStreamEntity(new FileInputStream(file));
		return sparkHttpClientHelper
				.createSparkURL(LITERAL.DATA, filePrefix)
				.getHttpResultContent(METHOD.POST, entity);
	}
	
	/**
	 * [DELETE] Deletes the specified file (only if under control of the JobServer)
	 * @param fileName
	 * @return
	 * @throws IOException 
	 * @throws UnsupportedOperationException 
	 */
	public String deleteData(String fileName) throws UnsupportedOperationException, IOException {
		LOGGER.info("Delete data {}", fileName);
		return sparkHttpClientHelper
				.createSparkURL(LITERAL.DATA, fileName)
				.getHttpResultContent(METHOD.DELETE);
	}
	
	/**
	 * [GET] Get HttpEntity with jar file binary stream.
	 * @param localJarPath
	 * @return
	 * @throws FileNotFoundException
	 */
	private HttpEntity getHttpEntityWithJar(String localJarPath) throws FileNotFoundException {
		File jarFile = new File(localJarPath);
		Preconditions.checkArgument(!jarFile.isDirectory(), LITERAL.ERROR.UPLOAD_ERROR01);
		Preconditions.checkArgument(jarFile.getName().endsWith(LITERAL.JAR_SUFFIX), LITERAL.ERROR.UPLOAD_ERROR02);
		return new InputStreamEntity(new FileInputStream(jarFile));
	}
	
	public static void main(String[] args) throws UnsupportedOperationException, IOException {
		SparkAccess sparkAccess = new SparkAccess("vm-45e5-3412", "8090");
		
//		System.out.println(sparkAccess.getJars());
//		System.out.println(sparkAccess.uploadJar("C:/Users/YM59750/job-server-demo_2.10-1.0.jar", "test3"));
		
//		System.out.println(sparkAccess.getContexts());
//		PersistentContextOptions options = new PersistentContextOptions(8, 1024);
//		options.addDependentJarUri("file:///sparkwordcount.jar");
//		LOGGER.info(sparkAccess.persistentContext("test-context", options));
		
//		System.out.println(sparkAccess.stopContext("test-context4"));
//		System.out.println(sparkAccess.resetContexts());
		
		StartJobOptions startJobOptions = new StartJobOptions("test", "WordCount");
		startJobOptions.setContext("test-context");
		startJobOptions.setSync(20000000);
		System.out.println(sparkAccess.startJob(startJobOptions, new StringEntity("input.string = a b c a b see")));
	
//		System.out.println(sparkAccess.getJobs());
//		System.out.println(sparkAccess.getJob("fdb65fb4-1fda-4380-b4b5-0ad0bce1850f"));
//		System.out.println(sparkAccess.killJob("80f23038-13b9-4e52-8a3b-225d84cc7540"));
//		System.out.println(sparkAccess.getJobConfig("fa663911-38f9-4f0c-8eaa-3aa58091e86c"));
//		System.out.println(sparkAccess.uploadData("test2.txt", "C:/Users/YM59750/Desktop/note-linux.txt"));
//		System.out.println(sparkAccess.deleteData("test.txt-2016-04-01T03_15_39.073-04_00.dat"));
//		System.out.println(sparkAccess.getData());
	}
}



package httpclient.spark;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import httpclient.base.BaseHttpAddress;
import model.spark.Options;

import com.google.common.base.Preconditions;

public class SparkHttpAddress extends BaseHttpAddress {

	private static final Logger LOGGER = LoggerFactory.getLogger(SparkHttpAddress.class);
	
	public SparkHttpAddress(String host, String port) {
		super(host, port);
	}
	
	public SparkHttpAddress(String protocol, String host, String port) {
		super(protocol, host, port);
	}

	public String createSparkURL(String... params) {
		Preconditions.checkArgument(params != null && params.length > 0, "Parameter can't be null/empty.");
		
		StringBuilder url = new StringBuilder(super.getBasicAddress());
		for (String param : params) {
			url.append("/").append(param);
		}
		setCurrentURL(url.toString());
		LOGGER.info("Create Spark URL: {}", getCurrentURL());
		return currentURL;
	}
	
	public String createSparkURL(Options options, String...params) {
		setCurrentURL(createSparkURL(params) + options.toString());
		LOGGER.info("Create Spark URL: {}", getCurrentURL());
		return getCurrentURL();
	}
	
}


package httpclient.spark;

import model.spark.Options;
import httpclient.base.BaseHttpAddress;
import httpclient.base.BaseHttpClientHelper;

public class SparkHttpClientHelper extends BaseHttpClientHelper {

	public SparkHttpClientHelper(BaseHttpAddress httpAddress) {
		super(httpAddress);
	}
	
	public SparkHttpClientHelper createSparkURL(String... params) {
		SparkHttpAddress sparkHttpAddress = (SparkHttpAddress)httpAddress;
		sparkHttpAddress.createSparkURL(params);
		return this;
	}
	
	public SparkHttpClientHelper createSparkURL(Options options, String... params) {
		SparkHttpAddress sparkHttpAddress = (SparkHttpAddress)httpAddress;
		sparkHttpAddress.createSparkURL(options, params);
		return this;
	}
}

package model.spark;

public abstract class Options {

}

package model.spark;

import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;


public class PersistentContextOptions extends Options {

	private int numCpuCores;
	private int memoryPerNode;
	private List<String> dependentJarUris;
	
	public PersistentContextOptions(int numCpuCores, int memoryPerNode) {
		this.numCpuCores = numCpuCores;
		this.memoryPerNode = memoryPerNode;
		this.dependentJarUris = new ArrayList<>();
	}
	
	public PersistentContextOptions() {}
	
	public int getNumCpuCores() {
		return numCpuCores;
	}

	public void setNumCpuCores(int numCpuCores) {
		this.numCpuCores = numCpuCores;
	}

	public int getMemoryPerNode() {
		return memoryPerNode;
	}

	public void setMemoryPerNode(int memoryPerNode) {
		this.memoryPerNode = memoryPerNode;
	}
	
	public void addDependentJarUri(String dependentJarUri) {
		if(this.dependentJarUris == null) {
			this.dependentJarUris = new ArrayList<>();
		}
		this.dependentJarUris.add(dependentJarUri);
	}
	
	@Override
	public String toString() {
		boolean andFlag = false;
		StringBuilder optionBuilder = new StringBuilder("?");
		
		if(numCpuCores > 0) {
			optionBuilder.append("num-cpu-cores=").append(numCpuCores);
			andFlag = true;
		}
		
		if(memoryPerNode > 0) {
			optionBuilder.append("&");
			optionBuilder.append("memory-per-node=").append(memoryPerNode).append("m");
			andFlag = true;
		}
		
		Iterator<String> uriIterable = dependentJarUris.iterator();
		if(andFlag && uriIterable.hasNext()) {
			optionBuilder.append("&");
		}
		if(uriIterable.hasNext()) {
			optionBuilder.append("dependent-jar-uris=");
		}
		while(uriIterable.hasNext()) {
			optionBuilder.append(uriIterable.next());
			if(uriIterable.hasNext()) {
				optionBuilder.append(",");
			}
		}
		return optionBuilder.toString();
	}
	
	public static void main(String[] args) {
		PersistentContextOptions options = new PersistentContextOptions();
		options.setMemoryPerNode(1024);
		options.setNumCpuCores(4);
		options.addDependentJarUri("hdfs://gft.hadoop.master:9000/jars/1.jar");
		options.addDependentJarUri("hdfs://gft.hadoop.master:9000/jars/2.jar");
		System.out.println(options);
	}
}

package model.spark;

import com.google.common.base.Preconditions;

public class StartJobOptions extends Options {

	private String appName;
	private String classPath;
	private String context;
	private boolean sync;
	private long timeout;
	
	public StartJobOptions(String appName, String classPath) {
		Preconditions.checkArgument(appName != null && !appName.equals(""), "appName can't be null/empty");
		Preconditions.checkArgument(classPath != null && !classPath.equals(""), "classPath can't be null/empty");
		
		this.appName = appName;
		this.classPath = classPath;
	}

	public String getAppName() {
		return appName;
	}

	public String getClassPath() {
		return classPath;
	}

	public void setContext(String context) {
		Preconditions.checkArgument(context != null && !context.equals(""), "context can't be null/empty");
		this.context = context;
	}
	
	public void setSync(long timeout) {
		this.timeout = timeout;
		this.sync = true;
	}
	
	@Override
	public String toString() {
		StringBuilder optionBuilder = new StringBuilder("?");
		optionBuilder.append("appName=").append(appName);
		optionBuilder.append("&");
		optionBuilder.append("classPath=").append(classPath);
		
		if(context != null) {
			optionBuilder.append("&context=").append(context);
		}
		if(sync == true)
			optionBuilder.append("&sync=").append(sync).append("&timeout=").append(timeout);
		
		return optionBuilder.toString();
	}
	
	public static void main(String[] args) {
		StartJobOptions startJobOptions = new StartJobOptions("test", "spark.jobserver.WordCountExample");
		startJobOptions.setContext("test-context");
		startJobOptions.setSync(1000);
		System.out.println(startJobOptions);
	}
}







